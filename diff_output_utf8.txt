diff --git a/app.py b/app.py
index ebb7db9..9165b29 100644
--- a/app.py
+++ b/app.py
@@ -1,5 +1,10 @@
 import argparse
 import os
+
+# 棰勫姞杞?PyTorch 鍜?cuDNN锛堝湪鎵€鏈夊叾浠栧鍏ヤ箣鍓嶏級
+import torch
+import onnxruntime as ort
+
 from demo.processor import IDPhotoProcessor
 from demo.ui import create_ui
 from hivision.creator.choose_handler import HUMAN_MATTING_MODELS
diff --git a/demo/assets/size_list_CN.csv b/demo/assets/size_list_CN.csv
index 0f24ded..bcff48a 100644
--- a/demo/assets/size_list_CN.csv
+++ b/demo/assets/size_list_CN.csv
@@ -1,19 +1,18 @@
 Name,Height,Width
-涓€瀵?413,295
-浜屽,626,413
-灏忎竴瀵?378,260
-灏忎簩瀵?531,413
-澶т竴瀵?567,390
-澶т簩瀵?626,413
-浜斿,1499,1050
-鏁欏笀璧勬牸璇?413,295
-鍥藉鍏姟鍛樿€冭瘯,413,295
-鍒濈骇浼氳鑰冭瘯,413,295
-鑻辫鍥涘叚绾ц€冭瘯,192,144
-璁＄畻鏈虹瓑绾ц€冭瘯,567,390
-鐮旂┒鐢熻€冭瘯,709,531
-绀句繚鍗?441,358
-鐢靛瓙椹鹃┒璇?378,260
-缇庡浗绛捐瘉,600,600
-鏃ユ湰绛捐瘉,413,295
-闊╁浗绛捐瘉,531,413
\ No newline at end of file
+鏍囧噯涓€瀵?3.5x2.5cm,413,295
+鏍囧噯浜屽 5.3x3.5cm,626,413
+灏忎竴瀵?3.2x2.2cm,378,260
+灏忎簩瀵?4.5x3.5cm,531,413
+澶т竴瀵?4.8x3.3cm,567,390
+澶т簩瀵?5.3x3.5cm,626,413
+浜斿 12.7x8.9cm,1499,1050
+鏁欏笀璧勬牸璇?3.5x2.5cm,413,295
+鍥藉鍏姟鍛樿€冭瘯 3.5x2.5cm,413,295
+鍒濈骇浼氳鑰冭瘯 3.5x2.5cm,413,295
+鑻辫鍥涘叚绾ц€冭瘯 4.8x3.3cm,567,390
+绀句繚鍗?3.2x2.6cm 浠呯數瀛愮増涓嶅彲鎵撳嵃,441,358
+鐢靛瓙椹鹃┒璇?3.2x2.2cm,378,260
+缇庡浗绛捐瘉 5.1x5.1cm,600,600
+鍙版棩鏂版嘲闊╂鍔犵璇?4.5x3.5cm,531,413
+涓浗鎶ょ収 4.8x3.3cm,567,390
+娓境閫氳 婢虫床绛捐瘉 4.8x3.3cm,567,390
\ No newline at end of file
diff --git a/demo/images/test0.jpg b/demo/images/test0.jpg
deleted file mode 100644
index bb7127a..0000000
Binary files a/demo/images/test0.jpg and /dev/null differ
diff --git a/demo/images/test1.jpg b/demo/images/test1.jpg
index 6b7ea4a..71a7ecb 100644
Binary files a/demo/images/test1.jpg and b/demo/images/test1.jpg differ
diff --git a/demo/images/test2.jpg b/demo/images/test2.jpg
deleted file mode 100644
index 4087473..0000000
Binary files a/demo/images/test2.jpg and /dev/null differ
diff --git a/demo/images/test3.jpg b/demo/images/test3.jpg
deleted file mode 100644
index f08f816..0000000
Binary files a/demo/images/test3.jpg and /dev/null differ
diff --git a/demo/images/test4.jpg b/demo/images/test4.jpg
deleted file mode 100644
index 16ea9fa..0000000
Binary files a/demo/images/test4.jpg and /dev/null differ
diff --git a/demo/locales.py b/demo/locales.py
index 571d3be..54bbd10 100644
--- a/demo/locales.py
+++ b/demo/locales.py
@@ -56,6 +56,24 @@ LOCALES = {
             "label": "毵ろ寘 氇嵏",
         },
     },
+    "process_mode": {
+        "en": {
+            "label": "Processing Mode",
+            "choices": ["Normal Mode", "Skip Background Removal"],
+        },
+        "zh": {
+            "label": "澶勭悊妯″紡",
+            "choices": ["姝ｅ父妯″紡", "璺宠繃鎶犲浘"],
+        },
+        "ja": {
+            "label": "鍑︾悊銉兗銉?,
+            "choices": ["閫氬父銉兗銉?, "鑳屾櫙闄ゅ幓銈掋偣銈儍銉?],
+        },
+        "ko": {
+            "label": "觳橂Μ 氇摐",
+            "choices": ["鞚茧皹 氇摐", "氚瓣步 鞝滉卑 瓯措剤霙瓣赴"],
+        },
+    },
     "key_param": {
         "en": {
             "label": "Key Parameters",
@@ -650,7 +668,7 @@ LOCALES = {
             "label": "銈枫儯銉笺儣鍖栧挤搴?,
         },
         "ko": {
-            "label": "靸ゃ兗銉?臧曤弰",
+            "label": "靸ゃ兗銉楆皶霃?,
         },
     },
     "saturation_strength": {
@@ -736,4 +754,4 @@ LOCALES = {
             "choices": ["6鞚胳箻", "5鞚胳箻", "A4", "3R", "4R"],
         },
     },
-}
+}
\ No newline at end of file
diff --git a/demo/processor.py b/demo/processor.py
index 1792407..d8a87ec 100644
--- a/demo/processor.py
+++ b/demo/processor.py
@@ -53,7 +53,7 @@ class IDPhotoProcessor:
         watermark_text_space,
         face_detect_option,
         head_measure_ratio=0.2,
-        top_distance_max=0.12,
+        top_distance_max=0.06,
         whitening_strength=0,
         image_dpi_option=False,
         custom_image_dpi=None,
@@ -63,9 +63,14 @@ class IDPhotoProcessor:
         saturation_strength=0,
         plugin_option=[],
         print_switch=None,
+        process_mode_options=None,
+        matting_sensitivity=0.95,
+        matting_resolution=1024,
     ):        
         # 鍒濆鍖栧弬鏁?         top_distance_min = top_distance_max - 0.02
+        # 璁剧疆澶勭悊妯″紡
+        crop_only = process_mode_options == LOCALES["process_mode"][language]["choices"][1]
         # 寰楀埌render_option鍦↙OCALES["render_mode"][language]["choices"]涓殑绱㈠紩
         render_option_index = LOCALES["render_mode"][language]["choices"].index(
             render_option
@@ -128,10 +133,14 @@ class IDPhotoProcessor:
 
         # 鍒涘缓IDCreator瀹炰緥骞惰缃鐞嗗櫒
         creator = IDCreator()
-        choose_handler(creator, matting_model_option, face_detect_option)
+        choose_handler(creator, matting_model_option, face_detect_option, 
+                      matting_sensitivity=matting_sensitivity, 
+                      matting_resolution=matting_resolution)
 
         # 鐢熸垚璇佷欢鐓?         try:
+            # 鏍规嵁澶勭悊妯″紡鍐冲畾鏄惁璺宠繃鎶犲浘
+            crop_only = process_mode_options == LOCALES["process_mode"][language]["choices"][1]
             result = self._generate_id_photo(
                 creator,
                 input_image,
@@ -146,8 +155,10 @@ class IDPhotoProcessor:
                 sharpen_strength,
                 saturation_strength,
                 face_alignment_option,
+                crop_only=crop_only,
             )
-        except (FaceError, APIError):
+        except (FaceError, APIError) as e:
+            print(f"Error during photo generation: {str(e)}")
             return self._handle_photo_generation_error(language)
 
         # 鍚庡鐞嗙敓鎴愮殑鐓х墖
@@ -292,6 +303,7 @@ class IDPhotoProcessor:
         sharpen_strength,
         saturation_strength,
         face_alignment_option,
+        crop_only=False,
     ):
         """鐢熸垚璇佷欢鐓?""
         change_bg_only = (
@@ -300,6 +312,7 @@ class IDPhotoProcessor:
         return creator(
             input_image,
             change_bg_only=change_bg_only,
+            crop_only=crop_only,
             size=idphoto_json["size"],
             head_measure_ratio=head_measure_ratio,
             head_top_range=(top_distance_max, top_distance_min),
@@ -317,7 +330,7 @@ class IDPhotoProcessor:
         return [gr.update(value=None) for _ in range(4)] + [
             gr.update(visible=False),
             gr.update(value=None),
-            gr.update(value=None),
+            gr.update(visible=False),
             gr.update(
                 value=LOCALES["notification"][language]["face_error"], visible=True
             ),
@@ -664,9 +677,10 @@ class IDPhotoProcessor:
     def _create_error_response(self, language):
         """鍒涘缓閿欒鍝嶅簲"""
         return [gr.update(value=None) for _ in range(4)] + [
-            None,
+            gr.update(visible=False),
+            gr.update(value=None),
+            gr.update(visible=False),
             gr.update(
                 value=LOCALES["size_mode"][language]["custom_size_eror"], visible=True
             ),
-            None,
         ]
diff --git a/demo/ui.py b/demo/ui.py
index 73895a3..ec9aaeb 100644
--- a/demo/ui.py
+++ b/demo/ui.py
@@ -33,7 +33,7 @@ def create_ui(
     else:
         DEFAULT_LANG = language[0]
 
-    DEFAULT_HUMAN_MATTING_MODEL = "modnet_photographic_portrait_matting"
+    DEFAULT_HUMAN_MATTING_MODEL = "hivision_modnet"
     DEFAULT_FACE_DETECT_MODEL = "retinaface-resnet50"
 
     if DEFAULT_HUMAN_MATTING_MODEL in human_matting_models:
@@ -71,6 +71,15 @@ def create_ui(
                         label=LOCALES["matting_model"][DEFAULT_LANG]["label"],
                         value=human_matting_models[0],
                     )
+                
+                with gr.Row():
+                    # 澶勭悊妯″紡閫夋嫨
+                    process_mode_options = gr.Radio(
+                        choices=LOCALES["process_mode"][DEFAULT_LANG]["choices"],
+                        label=LOCALES["process_mode"][DEFAULT_LANG]["label"],
+                        value=LOCALES["process_mode"][DEFAULT_LANG]["choices"][0],
+                        min_width=520,
+                    )
 
                 # TAB1 - 鍏抽敭鍙傛暟 ------------------------------------------------
                 with gr.Tab(
@@ -158,7 +167,7 @@ def create_ui(
                     head_measure_ratio_option = gr.Slider(
                         minimum=0.1,
                         maximum=0.5,
-                        value=0.2,
+                        value=0.22,
                         step=0.01,
                         label=LOCALES["head_measure_ratio"][DEFAULT_LANG]["label"],
                         interactive=True,
@@ -166,11 +175,28 @@ def create_ui(
                     top_distance_option = gr.Slider(
                         minimum=0.02,
                         maximum=0.5,
-                        value=0.12,
+                        value=0.08,
                         step=0.01,
                         label=LOCALES["top_distance"][DEFAULT_LANG]["label"],
                         interactive=True,
                     )
+                    
+                    # 鎶犲浘楂樼骇鍙傛暟
+                    with gr.Row():
+                        matting_sensitivity_option = gr.Slider(
+                            minimum=0.1,
+                            maximum=1.0,
+                            value=0.95,
+                            step=0.05,
+                            label="鎶犲浘鏁忔劅搴?/ Matting Sensitivity",
+                            interactive=True,
+                        )
+                        matting_resolution_option = gr.Dropdown(
+                            choices=[512, 1024, 2048],
+                            label="澶勭悊鍒嗚鲸鐜?/ Processing Resolution",
+                            value=1024,
+                            interactive=True,
+                        )
 
                     image_kb_options = gr.Radio(
                         choices=LOCALES["image_kb"][DEFAULT_LANG]["choices"],
@@ -425,6 +451,11 @@ def create_ui(
             # ---------------- 澶氳瑷€鍒囨崲鍑芥暟 ----------------
             def change_language(language):
                 return {
+                    process_mode_options: gr.update(
+                        label=LOCALES["process_mode"][language]["label"],
+                        choices=LOCALES["process_mode"][language]["choices"],
+                        value=LOCALES["process_mode"][language]["choices"][0],
+                    ),
                     face_detect_model_options: gr.update(
                         label=LOCALES["face_model"][language]["label"]
                     ),
@@ -644,6 +675,7 @@ def create_ui(
                 change_language,
                 inputs=[language_options],
                 outputs=[
+                    process_mode_options,
                     size_list_options,
                     mode_options,
                     color_options,
@@ -765,6 +797,9 @@ def create_ui(
                     saturation_option,
                     plugin_options,
                     print_options,
+                    process_mode_options,
+                    matting_sensitivity_option,
+                    matting_resolution_option,
                 ],
                 outputs=[
                     img_output_standard,
diff --git a/hivision/creator/__init__.py b/hivision/creator/__init__.py
index d6f9436..30e67db 100644
--- a/hivision/creator/__init__.py
+++ b/hivision/creator/__init__.py
@@ -107,6 +107,13 @@ class IDCreator:
         
         self.ctx = Context(params)
         ctx = self.ctx
+        
+        # 浼犻€掓姞鍥惧弬鏁板埌ctx
+        if hasattr(self, 'matting_sensitivity'):
+            ctx.matting_sensitivity = self.matting_sensitivity
+        if hasattr(self, 'matting_resolution'):
+            ctx.matting_resolution = self.matting_resolution
+            
         ctx.processing_image = image
         ctx.processing_image = U.resize_image_esp(
             ctx.processing_image, 2000
@@ -114,27 +121,34 @@ class IDCreator:
         ctx.origin_image = ctx.processing_image.copy()
         self.before_all and self.before_all(ctx)
 
-        # 1. ------------------浜哄儚鎶犲浘------------------
+        # 1. ------------------浜鸿劯妫€娴?-----------------
+        print("[1]  Start Face Detection...")
+        start_detection_time = time.time()
+        self.detection_handler(ctx)
+        end_detection_time = time.time()
+        print(f"[1]  Face Detection Time: {end_detection_time - start_detection_time:.3f}s")
+        self.after_detect and self.after_detect(ctx)
+
+        # 2. ------------------浜哄儚鎶犲浘------------------
         # 濡傛灉浠呰鍓紝鍒欎笉杩涜鎶犲浘
         if not ctx.params.crop_only:
             # 璋冪敤鎶犲浘宸ヤ綔娴?-            print("[1]  Start Human Matting...")
+            print("[2]  Start Human Matting...")
             start_matting_time = time.time()
             self.matting_handler(ctx)
             end_matting_time = time.time()
-            print(f"[1]  Human Matting Time: {end_matting_time - start_matting_time:.3f}s")
+            print(f"[2]  Human Matting Time: {end_matting_time - start_matting_time:.3f}s")
             self.after_matting and self.after_matting(ctx)
         # 濡傛灉杩涜鎶犲浘
         else:
             ctx.matting_image = ctx.processing_image
 
-
-        # 2. ------------------缇庨------------------
-        print("[2]  Start Beauty...")
+        # 3. ------------------缇庨------------------
+        print("[3]  Start Beauty...")
         start_beauty_time = time.time()
         self.beauty_handler(ctx)
         end_beauty_time = time.time()
-        print(f"[2]  Beauty Time: {end_beauty_time - start_beauty_time:.3f}s")
+        print(f"[3]  Beauty Time: {end_beauty_time - start_beauty_time:.3f}s")
 
         # 濡傛灉浠呮崲搴曪紝鍒欑洿鎺ヨ繑鍥炴姞鍥剧粨鏋?         if ctx.params.change_bg_only:
@@ -144,19 +158,11 @@ class IDCreator:
                 matting=ctx.matting_image,
                 clothing_params=None,
                 typography_params=None,
-                face=None,
+                face=ctx.face,
             )
             self.after_all and self.after_all(ctx)
             return ctx.result
 
-        # 3. ------------------浜鸿劯妫€娴?-----------------
-        print("[3]  Start Face Detection...")
-        start_detection_time = time.time()
-        self.detection_handler(ctx)
-        end_detection_time = time.time()
-        print(f"[3]  Face Detection Time: {end_detection_time - start_detection_time:.3f}s")
-        self.after_detect and self.after_detect(ctx)
-
         # 3.1 ------------------浜鸿劯瀵归綈------------------
         if ctx.params.face_alignment and abs(ctx.face["roll_angle"]) > 2:
             print("[3.1]  Start Face Alignment...")
diff --git a/hivision/creator/choose_handler.py b/hivision/creator/choose_handler.py
index d0885a0..485dc90 100644
--- a/hivision/creator/choose_handler.py
+++ b/hivision/creator/choose_handler.py
@@ -1,5 +1,6 @@
 from hivision.creator.human_matting import *
 from hivision.creator.face_detector import *
+from hivision.creator.human_matting import extract_human_rmbg_2
 
 
 HUMAN_MATTING_MODELS = [
@@ -7,18 +8,26 @@ HUMAN_MATTING_MODELS = [
     "birefnet-v1-lite",
     "hivision_modnet",
     "rmbg-1.4",
+    "rmbg-2.0",
 ]
 
 FACE_DETECT_MODELS = ["face++ (鑱旂綉Online API)", "mtcnn", "retinaface-resnet50"]
 
 
-def choose_handler(creator, matting_model_option=None, face_detect_option=None):
+def choose_handler(creator, matting_model_option=None, face_detect_option=None, 
+                  matting_sensitivity=0.95, matting_resolution=1024):
+    # 瀛樺偍鎶犲浘鍙傛暟鍒癱reator涓紝绋嶅悗鍦╟tx鍒涘缓鏃朵紶閫?+    creator.matting_sensitivity = matting_sensitivity
+    creator.matting_resolution = matting_resolution
+    
     if matting_model_option == "modnet_photographic_portrait_matting":
         creator.matting_handler = extract_human_modnet_photographic_portrait_matting
     elif matting_model_option == "mnn_hivision_modnet":
         creator.matting_handler = extract_human_mnn_modnet
     elif matting_model_option == "rmbg-1.4":
         creator.matting_handler = extract_human_rmbg
+    elif matting_model_option == "rmbg-2.0":
+        creator.matting_handler = extract_human_rmbg_2
     elif matting_model_option == "birefnet-v1-lite":
         creator.matting_handler = extract_human_birefnet_lite
     else:
diff --git a/hivision/creator/human_matting.py b/hivision/creator/human_matting.py
index 57b0d47..c82e366 100644
--- a/hivision/creator/human_matting.py
+++ b/hivision/creator/human_matting.py
@@ -32,6 +32,7 @@ WEIGHTS = {
         "mnn_hivision_modnet.mnn",
     ),
     "rmbg-1.4": os.path.join(os.path.dirname(__file__), "weights", "rmbg-1.4.onnx"),
+    "rmbg-2.0": os.path.join(os.path.dirname(__file__), "weights", "rmbg-2.0.onnx"),
     "birefnet-v1-lite": os.path.join(
         os.path.dirname(__file__), "weights", "birefnet-v1-lite.onnx"
     ),
@@ -39,12 +40,14 @@ WEIGHTS = {
 
 ONNX_DEVICE = onnxruntime.get_device()
 ONNX_PROVIDER = (
-    "CUDAExecutionProvider" if ONNX_DEVICE == "GPU" else "CPUExecutionProvider"
+    "CUDAExecutionProvider" if ONNX_DEVICE == "GPU" and "CUDAExecutionProvider" in onnxruntime.get_available_providers() else "CPUExecutionProvider"
 )
+print(f"ONNX Device: {ONNX_DEVICE}, Provider: {ONNX_PROVIDER}, Available providers: {onnxruntime.get_available_providers()}")
 
 HIVISION_MODNET_SESS = None
 MODNET_PHOTOGRAPHIC_PORTRAIT_MATTING_SESS = None
 RMBG_SESS = None
+RMBG_2_SESS = None
 BIREFNET_V1_LITE_SESS = None
 
 
@@ -62,14 +65,16 @@ def load_onnx_model(checkpoint_path, set_cpu=False):
     else:
         try:
             sess = onnxruntime.InferenceSession(checkpoint_path, providers=providers)
+            print(f"Model {os.path.basename(checkpoint_path)} loaded with providers: {sess.get_providers()}")
         except Exception as e:
-            if ONNX_DEVICE == "CUDAExecutionProvider":
+            if ONNX_PROVIDER == "CUDAExecutionProvider":  # 淇锛氫娇鐢∣NNX_PROVIDER鑰屼笉鏄疧NNX_DEVICE
                 print(f"Failed to load model with CUDAExecutionProvider: {e}")
                 print("Falling back to CPUExecutionProvider")
                 # 灏濊瘯浣跨敤CPU鍔犺浇妯″瀷
                 sess = onnxruntime.InferenceSession(
                     checkpoint_path, providers=["CPUExecutionProvider"]
                 )
+                print(f"Model {os.path.basename(checkpoint_path)} loaded with providers: {sess.get_providers()}")
             else:
                 raise e  # 濡傛灉鏄疌PU鎵ц澶辫触锛岄噸鏂版姏鍑哄紓甯? 
@@ -83,6 +88,8 @@ def extract_human(ctx: Context):
     """
     # 鎶犲浘
     matting_image = get_modnet_matting(ctx.processing_image, WEIGHTS["hivision_modnet"])
+    if matting_image is None:
+        raise RuntimeError("HivisionIDPhotos ModNet 妯″瀷鍔犺浇澶辫触鎴栨ā鍨嬫枃浠朵笉瀛樺湪锛岃妫€鏌ユā鍨嬫枃浠惰矾寰?)
     # 淇鎶犲浘
     ctx.processing_image = hollow_out_fix(matting_image)
     ctx.matting_image = ctx.processing_image.copy()
@@ -97,6 +104,8 @@ def extract_human_modnet_photographic_portrait_matting(ctx: Context):
     matting_image = get_modnet_matting_photographic_portrait_matting(
         ctx.processing_image, WEIGHTS["modnet_photographic_portrait_matting"]
     )
+    if matting_image is None:
+        raise RuntimeError("ModNet Photographic Portrait 妯″瀷鍔犺浇澶辫触鎴栨ā鍨嬫枃浠朵笉瀛樺湪锛岃妫€鏌ユā鍨嬫枃浠惰矾寰?)
     # 淇鎶犲浘
     ctx.processing_image = matting_image
     ctx.matting_image = ctx.processing_image.copy()
@@ -106,12 +115,29 @@ def extract_human_mnn_modnet(ctx: Context):
     matting_image = get_mnn_modnet_matting(
         ctx.processing_image, WEIGHTS["mnn_hivision_modnet"]
     )
+    if matting_image is None:
+        raise RuntimeError("MNN ModNet 妯″瀷鍔犺浇澶辫触鎴栨ā鍨嬫枃浠朵笉瀛樺湪锛岃妫€鏌ユā鍨嬫枃浠惰矾寰?)
     ctx.processing_image = hollow_out_fix(matting_image)
     ctx.matting_image = ctx.processing_image.copy()
 
 
 def extract_human_rmbg(ctx: Context):
     matting_image = get_rmbg_matting(ctx.processing_image, WEIGHTS["rmbg-1.4"])
+    if matting_image is None:
+        raise RuntimeError("RMBG 1.4 妯″瀷鍔犺浇澶辫触鎴栨ā鍨嬫枃浠朵笉瀛樺湪锛岃妫€鏌ユā鍨嬫枃浠惰矾寰?)
+    ctx.processing_image = matting_image
+    ctx.matting_image = ctx.processing_image.copy()
+
+
+def extract_human_rmbg_2(ctx: Context):
+    # 鑾峰彇鎶犲浘鍙傛暟
+    sensitivity = getattr(ctx, 'matting_sensitivity', 0.95)
+    resolution = getattr(ctx, 'matting_resolution', 1024)
+    
+    matting_image = get_rmbg_2_matting(ctx.processing_image, WEIGHTS["rmbg-2.0"], 
+                                      ref_size=resolution, sensitivity=sensitivity)
+    if matting_image is None:
+        raise RuntimeError("RMBG 2.0 妯″瀷鍔犺浇澶辫触鎴栨ā鍨嬫枃浠朵笉瀛樺湪锛岃妫€鏌ユā鍨嬫枃浠惰矾寰?)
     ctx.processing_image = matting_image
     ctx.matting_image = ctx.processing_image.copy()
 
@@ -128,6 +154,8 @@ def extract_human_birefnet_lite(ctx: Context):
     matting_image = get_birefnet_portrait_matting(
         ctx.processing_image, WEIGHTS["birefnet-v1-lite"]
     )
+    if matting_image is None:
+        raise RuntimeError("BiRefNet Lite 妯″瀷鍔犺浇澶辫触鎴栨ā鍨嬫枃浠朵笉瀛樺湪锛岃妫€鏌ユā鍨嬫枃浠惰矾寰?)
     ctx.processing_image = matting_image
     ctx.matting_image = ctx.processing_image.copy()
 
@@ -203,7 +231,7 @@ def get_modnet_matting(input_image, checkpoint_path, ref_size=512):
 
     # 濡傛灉RUN_MODE涓嶆槸閲庡吔妯″紡锛屽垯涓嶅姞杞芥ā鍨?     if HIVISION_MODNET_SESS is None:
-        HIVISION_MODNET_SESS = load_onnx_model(checkpoint_path, set_cpu=True)
+        HIVISION_MODNET_SESS = load_onnx_model(checkpoint_path)
 
     input_name = HIVISION_MODNET_SESS.get_inputs()[0].name
     output_name = HIVISION_MODNET_SESS.get_outputs()[0].name
@@ -237,7 +265,7 @@ def get_modnet_matting_photographic_portrait_matting(
     # 濡傛灉RUN_MODE涓嶆槸閲庡吔妯″紡锛屽垯涓嶅姞杞芥ā鍨?     if MODNET_PHOTOGRAPHIC_PORTRAIT_MATTING_SESS is None:
         MODNET_PHOTOGRAPHIC_PORTRAIT_MATTING_SESS = load_onnx_model(
-            checkpoint_path, set_cpu=True
+            checkpoint_path
         )
 
     input_name = MODNET_PHOTOGRAPHIC_PORTRAIT_MATTING_SESS.get_inputs()[0].name
@@ -276,7 +304,7 @@ def get_rmbg_matting(input_image: np.ndarray, checkpoint_path, ref_size=1024):
         return image
 
     if RMBG_SESS is None:
-        RMBG_SESS = load_onnx_model(checkpoint_path, set_cpu=True)
+        RMBG_SESS = load_onnx_model(checkpoint_path)
 
     orig_image = Image.fromarray(input_image)
     image = resize_rmbg_image(orig_image)
@@ -295,14 +323,15 @@ def get_rmbg_matting(input_image: np.ndarray, checkpoint_path, ref_size=1024):
     mi = np.min(result)
     result = (result - mi) / (ma - mi)  # Normalize to [0, 1]
 
-    # Convert to PIL image
+    # Convert to PIL image with enhanced edge processing
     im_array = (result * 255).astype(np.uint8)
     pil_im = Image.fromarray(
         im_array, mode="L"
     )  # Ensure mask is single channel (L mode)
 
     # Resize the mask to match the original image size
-    pil_im = pil_im.resize(orig_image.size, Image.BILINEAR)
+    # 浣跨敤LANCZOS鎻掑€间互鑾峰緱鏇村ソ鐨勮竟缂樿川閲忥紝鐗瑰埆鏄缁嗗井鍙戜笣
+    pil_im = pil_im.resize(orig_image.size, Image.LANCZOS)
 
     # Paste the mask on the original image
     new_im = Image.new("RGBA", orig_image.size, (0, 0, 0, 0))
@@ -315,6 +344,108 @@ def get_rmbg_matting(input_image: np.ndarray, checkpoint_path, ref_size=1024):
     return np.array(new_im)
 
 
+def get_rmbg_2_matting(input_image: np.ndarray, checkpoint_path, ref_size=1024, sensitivity=0.95):
+    """
+    RMBG 2.0 鎶犲浘澶勭悊鍑芥暟
+    鍩轰簬BiRefNet鏋舵瀯锛屼娇鐢ㄤ笉鍚岀殑棰勫鐞嗗弬鏁?+    """
+    global RMBG_2_SESS
+
+    if not os.path.exists(checkpoint_path):
+        print(f"Checkpoint file not found: {checkpoint_path}")
+        return None
+
+    def resize_rmbg_2_image(image):
+        image = image.convert("RGB")
+        model_input_size = (ref_size, ref_size)
+        image = image.resize(model_input_size, Image.BILINEAR)
+        return image
+
+    # 璁板綍鍔犺浇onnx妯″瀷鐨勫紑濮嬫椂闂?+    load_start_time = time()
+
+    # 濡傛灉RUN_MODE涓嶆槸閲庡吔妯″紡锛屽垯涓嶅姞杞芥ā鍨?+    if RMBG_2_SESS is None:
+        # print("棣栨鍔犺浇rmbg-2.0妯″瀷...")
+        if ONNX_DEVICE == "GPU":
+            print("onnxruntime-gpu宸插畨瑁咃紝灏濊瘯浣跨敤CUDA鍔犺浇RMBG 2.0妯″瀷")
+            try:
+                import torch
+            except ImportError:
+                print(
+                    "torch鏈畨瑁咃紝灏濊瘯鐩存帴浣跨敤onnxruntime-gpu鍔犺浇妯″瀷锛岃繖闇€瑕侀厤缃ソCUDA鍜宑uDNN"
+                )
+            RMBG_2_SESS = load_onnx_model(checkpoint_path)
+        else:
+            RMBG_2_SESS = load_onnx_model(checkpoint_path)
+
+    # 璁板綍鍔犺浇onnx妯″瀷鐨勭粨鏉熸椂闂?+    load_end_time = time()
+
+    # 鎵撳嵃鍔犺浇onnx妯″瀷鎵€鑺辩殑鏃堕棿
+    print(f"Loading RMBG 2.0 ONNX model took {load_end_time - load_start_time:.4f} seconds")
+
+    orig_image = Image.fromarray(input_image)
+    image = resize_rmbg_2_image(orig_image)
+    
+    # RMBG 2.0 浣跨敤鏍囧噯鐨処mageNet棰勫鐞嗗弬鏁?+    im_np = np.array(image).astype(np.float32)
+    im_np = im_np.transpose(2, 0, 1)  # Change to CxHxW format
+    im_np = np.expand_dims(im_np, axis=0)  # Add batch dimension
+    im_np = im_np / 255.0  # Normalize to [0, 1]
+    
+    # 浣跨敤ImageNet鏍囧噯鍖栧弬鏁?(涓嶣iRefNet鏋舵瀯涓€鑷?
+    mean = np.array([0.485, 0.456, 0.406]).reshape(1, 3, 1, 1)
+    std = np.array([0.229, 0.224, 0.225]).reshape(1, 3, 1, 1)
+    im_np = (im_np - mean) / std
+
+    input_name = RMBG_2_SESS.get_inputs()[0].name
+    print(onnxruntime.get_device(), RMBG_2_SESS.get_providers())
+
+    time_st = time()
+    # Inference
+    result = RMBG_2_SESS.run(None, {input_name: im_np.astype(np.float32)})[0]
+    print(f"RMBG 2.0 Inference time: {time() - time_st:.4f} seconds")
+
+    # Post process - RMBG 2.0 鍚庡鐞?+    result = np.squeeze(result)
+    
+    # 妫€鏌ヨ緭鍑鸿寖鍥达紝濡傛灉鍦╗-inf, inf]鑼冨洿鍐呭垯闇€瑕乻igmoid锛屽鏋滃湪[0,1]鍒欏凡缁忚繃sigmoid
+    if np.min(result) < 0 or np.max(result) > 1:
+        # 搴旂敤sigmoid鍑芥暟锛堟ā鍨嬭緭鍑烘湭缁忚繃sigmoid锛?+        result = 1 / (1 + np.exp(-result))  # Sigmoid function
+    
+    # 搴旂敤鏁忔劅搴﹁皟鏁?- 鏀瑰杽缁嗗井鍙戜笣澶勭悊
+    # 鏁忔劅搴﹁秺楂橈紝淇濈暀鏇村缁嗚妭锛涙晱鎰熷害瓒婁綆锛屾洿鍔犲钩婊?+    if sensitivity != 1.0:
+        # 浣跨敤骞傚嚱鏁拌皟鏁存晱鎰熷害锛屼繚鎸佺粏寰彂涓濈殑缁嗚妭
+        gamma = 2.0 - sensitivity  # sensitivity=0.95鏃讹紝gamma=1.05锛泂ensitivity=0.1鏃讹紝gamma=1.9
+        result = np.power(result, gamma)
+    
+    # 纭繚缁撴灉鍦╗0,1]鑼冨洿鍐?+    result = np.clip(result, 0, 1)
+
+    # Convert to PIL image with enhanced edge processing
+    im_array = (result * 255).astype(np.uint8)
+    pil_im = Image.fromarray(
+        im_array, mode="L"
+    )  # Ensure mask is single channel (L mode)
+
+    # Resize the mask to match the original image size
+    # 浣跨敤LANCZOS鎻掑€间互鑾峰緱鏇村ソ鐨勮竟缂樿川閲忥紝鐗瑰埆鏄缁嗗井鍙戜笣
+    pil_im = pil_im.resize(orig_image.size, Image.LANCZOS)
+
+    # Paste the mask on the original image
+    new_im = Image.new("RGBA", orig_image.size, (0, 0, 0, 0))
+    new_im.paste(orig_image, mask=pil_im)
+    
+    # 濡傛灉RUN_MODE涓嶆槸閲庡吔妯″紡锛屽垯閲婃斁妯″瀷
+    if os.getenv("RUN_MODE") != "beast":
+        RMBG_2_SESS = None
+
+    return np.array(new_im)
+
+
 def get_mnn_modnet_matting(input_image, checkpoint_path, ref_size=512):
     if not os.path.exists(checkpoint_path):
         print(f"Checkpoint file not found: {checkpoint_path}")
@@ -389,7 +520,7 @@ def get_birefnet_portrait_matting(input_image, checkpoint_path, ref_size=512):
                 )
             BIREFNET_V1_LITE_SESS = load_onnx_model(checkpoint_path)
         else:
-            BIREFNET_V1_LITE_SESS = load_onnx_model(checkpoint_path, set_cpu=True)
+            BIREFNET_V1_LITE_SESS = load_onnx_model(checkpoint_path)
 
     # 璁板綍鍔犺浇onnx妯″瀷鐨勭粨鏉熸椂闂?     load_end_time = time()
diff --git a/hivision/creator/layout_calculator.py b/hivision/creator/layout_calculator.py
index 4576314..a7a6480 100644
--- a/hivision/creator/layout_calculator.py
+++ b/hivision/creator/layout_calculator.py
@@ -32,14 +32,14 @@ def judge_layout(
     # 1.涓嶈浆缃帓鍒楃殑鎯呭喌涓嬶細
     layout_col_no_transpose = 0  # 琛?     layout_row_no_transpose = 0  # 鍒?-    for i in range(1, 4):
+    for i in range(1, 4):  # 鏈€澶?琛?         centerBlockHeight_temp = input_height * i + PHOTO_INTERVAL_H * (i - 1)
         if centerBlockHeight_temp < LIMIT_BLOCK_H:
             centerBlockHeight_1 = centerBlockHeight_temp
             layout_row_no_transpose = i
         else:
             break
-    for j in range(1, 9):
+    for j in range(1, 4):  # 鏈€澶?鍒楋紝闄愬埗涓?x3
         centerBlockWidth_temp = input_width * j + PHOTO_INTERVAL_W * (j - 1)
         if centerBlockWidth_temp < LIMIT_BLOCK_W:
             centerBlockWidth_1 = centerBlockWidth_temp
@@ -51,14 +51,14 @@ def judge_layout(
     # 2.杞疆鎺掑垪鐨勬儏鍐典笅锛?     layout_col_transpose = 0  # 琛?     layout_row_transpose = 0  # 鍒?-    for i in range(1, 4):
+    for i in range(1, 4):  # 鏈€澶?琛?         centerBlockHeight_temp = input_width * i + PHOTO_INTERVAL_H * (i - 1)
         if centerBlockHeight_temp < LIMIT_BLOCK_H:
             centerBlockHeight_2 = centerBlockHeight_temp
             layout_row_transpose = i
         else:
             break
-    for j in range(1, 9):
+    for j in range(1, 4):  # 鏈€澶?鍒楋紝闄愬埗涓?x3
         centerBlockWidth_temp = input_height * j + PHOTO_INTERVAL_W * (j - 1)
         if centerBlockWidth_temp < LIMIT_BLOCK_W:
             centerBlockWidth_2 = centerBlockWidth_temp
@@ -77,9 +77,9 @@ def judge_layout(
 
 def generate_layout_array(input_height, input_width, LAYOUT_WIDTH=1795, LAYOUT_HEIGHT=1205):
     # 1.鍩虹鍙傛暟琛?-    PHOTO_INTERVAL_H = 30  # 璇佷欢鐓т笌璇佷欢鐓т箣闂寸殑鍨傜洿璺濈
-    PHOTO_INTERVAL_W = 30  # 璇佷欢鐓т笌璇佷欢鐓т箣闂寸殑姘村钩璺濈
-    SIDES_INTERVAL_H = 50  # 璇佷欢鐓т笌鐢诲竷杈圭紭鐨勫瀭鐩磋窛绂?+    PHOTO_INTERVAL_H = 0  # 璇佷欢鐓т笌璇佷欢鐓т箣闂寸殑鍨傜洿璺濈
+    PHOTO_INTERVAL_W = 0  # 璇佷欢鐓т笌璇佷欢鐓т箣闂寸殑姘村钩璺濈
+    SIDES_INTERVAL_H = 70  # 璇佷欢鐓т笌鐢诲竷杈圭紭鐨勫瀭鐩磋窛绂?     SIDES_INTERVAL_W = 70  # 璇佷欢鐓т笌鐢诲竷杈圭紭鐨勬按骞宠窛绂?     LIMIT_BLOCK_W = LAYOUT_WIDTH - 2 * SIDES_INTERVAL_W
     LIMIT_BLOCK_H = LAYOUT_HEIGHT - 2 * SIDES_INTERVAL_H
@@ -146,33 +146,76 @@ def generate_layout_image(
         )
 
     if crop_line:
-        # 娣诲姞瑁佸壀绾?-        line_color = (200, 200, 200)  # 娴呯伆鑹?+        # 娣诲姞瑁佸壀绾匡紙鍙湪鐓х墖澶栧洿鏄剧ず锛?+        line_color = (100, 100, 100)  # 娣辩伆鑹?         line_thickness = 1
 
-        # 鍒濆鍖栬鍓嚎浣嶇疆鍒楄〃
-        vertical_lines = []
-        horizontal_lines = []
-
-        # 鏍规嵁鎺掔増鏁扮粍娣诲姞瑁佸壀绾?+        # 鏀堕泦鎵€鏈夌収鐗囩殑浣嶇疆淇℃伅
+        photo_positions = []
         for arr in typography_arr:
             x, y = arr[0], arr[1]
-            if x not in vertical_lines:
-                vertical_lines.append(x)
-            if x + width not in vertical_lines:
-                vertical_lines.append(x + width)
-            if y not in horizontal_lines:
-                horizontal_lines.append(y)
-            if y + height not in horizontal_lines:
-                horizontal_lines.append(y + height)
-
-        # 缁樺埗鍨傜洿瑁佸壀绾?-        for x in vertical_lines:
-            cv2.line(white_background, (x, 0), (x, LAYOUT_HEIGHT), line_color, line_thickness)
-
-        # 缁樺埗姘村钩瑁佸壀绾?-        for y in horizontal_lines:
-            cv2.line(white_background, (0, y), (LAYOUT_WIDTH, y), line_color, line_thickness)
+            photo_positions.append((x, y, x + width, y + height))
+
+        # 鑾峰彇鎵€鏈夊瀭鐩村拰姘村钩杈圭晫绾?+        vertical_lines = set()
+        horizontal_lines = set()
+        for x, y, x2, y2 in photo_positions:
+            vertical_lines.add(x)
+            vertical_lines.add(x2)
+            horizontal_lines.add(y)
+            horizontal_lines.add(y2)
+
+        # 缁樺埗鍨傜洿瑁佸壀绾匡紙閬垮紑鐓х墖鍐呴儴锛?+        for x in sorted(vertical_lines):
+            # 鎵惧埌璇ュ瀭鐩寸嚎涓婅鐓х墖鍗犵敤鐨勫尯闂?+            occupied_ranges = []
+            for px, py, px2, py2 in photo_positions:
+                if px <= x <= px2:
+                    occupied_ranges.append((py, py2))
+            
+            # 鍚堝苟閲嶅彔鍖洪棿
+            occupied_ranges.sort()
+            merged_ranges = []
+            for start, end in occupied_ranges:
+                if merged_ranges and start <= merged_ranges[-1][1]:
+                    merged_ranges[-1] = (merged_ranges[-1][0], max(merged_ranges[-1][1], end))
+                else:
+                    merged_ranges.append((start, end))
+            
+            # 缁樺埗闈炲崰鐢ㄥ尯鍩熺殑绾挎
+            current_y = 0
+            for start, end in merged_ranges:
+                if current_y < start:
+                    cv2.line(white_background, (x, current_y), (x, start), line_color, line_thickness)
+                current_y = end
+            if current_y < LAYOUT_HEIGHT:
+                cv2.line(white_background, (x, current_y), (x, LAYOUT_HEIGHT), line_color, line_thickness)
+
+        # 缁樺埗姘村钩瑁佸壀绾匡紙閬垮紑鐓х墖鍐呴儴锛?+        for y in sorted(horizontal_lines):
+            # 鎵惧埌璇ユ按骞崇嚎涓婅鐓х墖鍗犵敤鐨勫尯闂?+            occupied_ranges = []
+            for px, py, px2, py2 in photo_positions:
+                if py <= y <= py2:
+                    occupied_ranges.append((px, px2))
+            
+            # 鍚堝苟閲嶅彔鍖洪棿
+            occupied_ranges.sort()
+            merged_ranges = []
+            for start, end in occupied_ranges:
+                if merged_ranges and start <= merged_ranges[-1][1]:
+                    merged_ranges[-1] = (merged_ranges[-1][0], max(merged_ranges[-1][1], end))
+                else:
+                    merged_ranges.append((start, end))
+            
+            # 缁樺埗闈炲崰鐢ㄥ尯鍩熺殑绾挎
+            current_x = 0
+            for start, end in merged_ranges:
+                if current_x < start:
+                    cv2.line(white_background, (current_x, y), (start, y), line_color, line_thickness)
+                current_x = end
+            if current_x < LAYOUT_WIDTH:
+                cv2.line(white_background, (current_x, y), (LAYOUT_WIDTH, y), line_color, line_thickness)
 
     # 杩斿洖鎺掔増鍚庣殑鍥惧儚
     return white_background
diff --git a/hivision/creator/photo_adjuster.py b/hivision/creator/photo_adjuster.py
index 2e966a7..22c87fd 100644
--- a/hivision/creator/photo_adjuster.py
+++ b/hivision/creator/photo_adjuster.py
@@ -24,7 +24,35 @@ def adjust_photo(ctx: Context):
     w, h = face_rect[2], face_rect[3]
     height, width = ctx.matting_image.shape[:2]
     width_height_ratio = standard_size[0] / standard_size[1]
-    # Step2. 璁＄畻楂樼骇鍙傛暟
+    
+    # Step2. 鍏堣幏鍙栧ご椤跺疄闄呬綅缃?+    # 妫€鏌ユ槸鍚︿负crop_only妯″紡锛堟病鏈夎繘琛屾姞鍥撅級
+    if ctx.params.crop_only:
+        # 鍦╟rop_only妯″紡涓嬶紝闇€瑕佽繘琛屼复鏃舵姞鍥炬潵鑾峰緱姝ｇ‘鐨勫ご椤朵綅缃?+        from .human_matting import get_modnet_matting, WEIGHTS
+        
+        # 杩涜涓存椂鎶犲浘鑾峰彇alpha閫氶亾
+        temp_matting = get_modnet_matting(ctx.processing_image, WEIGHTS["hivision_modnet"])
+        _, _, _, alpha = cv2.split(temp_matting)
+        _, alpha = cv2.threshold(alpha, 127, 255, cv2.THRESH_BINARY)
+    else:
+        # 姝ｅ父鎶犲浘妯″紡锛岀洿鎺ヤ娇鐢╩atting_image鐨刟lpha閫氶亾
+        _, _, _, alpha = cv2.split(ctx.matting_image)
+        _, alpha = cv2.threshold(alpha, 127, 255, cv2.THRESH_BINARY)
+    
+    # 鍦ㄩ潰閮ㄥ尯鍩熶笂鏂瑰鎵惧ご椤?+    search_start = max(0, int(y - h * 0.8))  # 浠庨潰閮ㄤ笂鏂瑰紑濮嬫悳绱?+    search_end = int(y)  # 鎼滅储鍒伴潰閮ㄩ《閮?+    face_left = max(0, int(x - w * 0.2))  # 闈㈤儴宸︿晶鎵╁睍
+    face_right = min(width, int(x + w + w * 0.2))  # 闈㈤儴鍙充晶鎵╁睍
+    
+    head_top = search_start
+    for i in range(search_start, search_end):
+        if np.any(alpha[i, face_left:face_right] > 0):
+            head_top = i
+            break
+    
+    # Step3. 璁＄畻楂樼骇鍙傛暟
     face_center = (x + w / 2, y + h / 2)  # 闈㈤儴涓績鍧愭爣
     face_measure = w * h  # 闈㈤儴闈㈢Н
     crop_measure = (
@@ -39,58 +67,27 @@ def adjust_photo(ctx: Context):
         int(standard_size[1] * resize_ratio_single),
     )  # 瑁佸壀妗嗗ぇ灏? 
-    # 瑁佸壀妗嗙殑瀹氫綅淇℃伅
+    # 璁＄畻瑁佸壀妗嗕綅缃?+    # 璁＄畻澶撮《绌洪棿锛堝熀浜巋ead_top_range鍙傛暟锛?+    top_distance_max, top_distance_min = params.head_top_range
+    # 浣跨敤top_distance_max浣滀负澶撮《璺濈姣斾緥
+    desired_top_space = int(crop_size[0] * top_distance_max)  # 澶撮《绌洪棿涓鸿鍓楂樺害鐨勬瘮渚?+    
+    # 璁＄畻瑁佸壀妗嗙殑y鍧愭爣
+    y1 = int(head_top - desired_top_space)  # 鍩轰簬澶撮《浣嶇疆鍜屽弬鏁板寲绌洪棿璁＄畻y1
+    y2 = y1 + crop_size[0]  # 璁＄畻y2
+    
+    # 璁＄畻瑁佸壀妗嗙殑x鍧愭爣锛堜繚鎸佹按骞冲眳涓級
     x1 = int(face_center[0] - crop_size[1] / 2)
-    y1 = int(face_center[1] - crop_size[0] * params.head_height_ratio)
-    y2 = y1 + crop_size[0]
     x2 = x1 + crop_size[1]
 
-    # Step3, 瑁佸壀妗嗙殑璋冩暣
-    cut_image = IDphotos_cut(x1, y1, x2, y2, ctx.matting_image)
-    cut_image = cv2.resize(cut_image, (crop_size[1], crop_size[0]))
-    y_top, y_bottom, x_left, x_right = U.get_box(
-        cut_image.astype(np.uint8), model=2, correction_factor=0
-    )  # 寰楀埌 cut_image 涓汉鍍忕殑涓婁笅宸﹀彸璺濈淇℃伅
-
-    # Step5. 鍒ゅ畾 cut_image 涓殑浜哄儚鏄惁澶勪簬鍚堢悊鐨勪綅缃紝鑻ヤ笉鍚堢悊锛屽垯澶勭悊鏁版嵁浠ヤ究涔嬪悗璋冩暣浣嶇疆
-    # 妫€娴嬩汉鍍忎笌瑁佸壀妗嗗乏杈规垨鍙宠竟鏄惁瀛樺湪绌洪殭
-    if x_left > 0 or x_right > 0:
-        status_left_right = 1
-        cut_value_top = int(
-            ((x_left + x_right) * width_height_ratio) / 2
-        )  # 鍑忓幓宸﹀彸锛屼负浜嗕繚鎸佹瘮渚嬶紝涓婁笅涔熻鐩稿簲鍑忓皯 cut_value_top
-    else:
-        status_left_right = 0
-        cut_value_top = 0
-
-    """
-        妫€娴嬩汉澶撮《涓庣収鐗囩殑椤堕儴鏄惁鍦ㄥ悎閫傜殑璺濈鍐咃細
-        - status==0: 璺濈鍚堥€傦紝鏃犻渶绉诲姩
-        - status=1: 璺濈杩囧ぇ锛屼汉鍍忓簲鍚戜笂绉诲姩
-        - status=2: 璺濈杩囧皬锛屼汉鍍忓簲鍚戜笅绉诲姩
-    """
-    status_top, move_value = U.detect_distance(
-        y_top - cut_value_top,
-        crop_size[0],
-        max=params.head_top_range[0],
-        min=params.head_top_range[1],
-    )
-
-    # Step6. 瀵圭収鐗囩殑绗簩杞鍓?-    if status_left_right == 0 and status_top == 0:
-        result_image = cut_image
-    else:
-        result_image = IDphotos_cut(
-            x1 + x_left,
-            y1 + cut_value_top + status_top * move_value,
-            x2 - x_right,
-            y2 - cut_value_top + status_top * move_value,
-            ctx.matting_image,
-        )
+    # Step3, 瑁佸壀鍥惧儚
+    result_image = IDphotos_cut(x1, y1, x2, y2, ctx.matting_image)
+    result_image = cv2.resize(result_image, (crop_size[1], crop_size[0]))
 
     # 鎹㈣鍙傛暟鍑嗗
-    relative_x = x - (x1 + x_left)
-    relative_y = y - (y1 + cut_value_top + status_top * move_value)
+    relative_x = x - x1
+    relative_y = y - y1
 
     # Step7. 褰撶収鐗囧簳閮ㄥ瓨鍦ㄧ┖闅欐椂锛屼笅鎷夎嚦搴曢儴
     result_image, y_high = move(result_image.astype(np.uint8))
@@ -188,15 +185,33 @@ def IDphotos_cut(x1, y1, x2, y2, img):
 
 def move(input_image):
     """
-    瑁佸壀涓诲嚱鏁帮紝杈撳叆涓€寮?png 鍥惧儚锛岃鍥惧儚鍛ㄥ洿鏄€忔槑鐨?+    瑁佸壀涓诲嚱鏁帮紝杈撳叆涓€寮?png 鍥惧儚锛岃鍥惧儚鍛ㄥ洿鏄€忔槑鐨勩€?+    灏嗗浘鍍忓簳閮ㄧ殑閫忔槑閮ㄥ垎绉诲姩鍒伴《閮紝浣夸汉鍍忚创杩戝簳閮ㄣ€?     """
-    png_img = input_image  # 鑾峰彇鍥惧儚
-
-    height, width, channels = png_img.shape  # 楂?y銆佸 x
-    y_low, y_high, _, _ = U.get_box(png_img, model=2)  # for 寰幆
-    base = np.zeros((y_high, width, channels), dtype=np.uint8)  # for 寰幆
-    png_img = png_img[0 : height - y_high, :, :]  # for 寰幆
-    png_img = np.concatenate((base, png_img), axis=0)
+    png_img = input_image.copy()
+    height, width, channels = png_img.shape
+    
+    # 鑾峰彇閫忔槑閫氶亾
+    _, _, _, alpha = cv2.split(png_img)
+    
+    # 鎵惧埌搴曢儴鏈€鍚庝竴涓潪閫忔槑鍍忕礌鐨勪綅缃?+    bottom_pos = height - 1
+    for i in range(height - 1, -1, -1):
+        if np.any(alpha[i, :] > 127):
+            bottom_pos = i
+            break
+    
+    # 璁＄畻搴曢儴绌虹櫧鐨勯珮搴?+    y_high = height - bottom_pos - 1
+    
+    if y_high > 0:
+        # 鍒涘缓椤堕儴绌虹櫧鍖哄煙
+        base = np.zeros((y_high, width, channels), dtype=np.uint8)
+        # 瑁佸壀鎺夊簳閮ㄧ┖鐧?+        png_img = png_img[0:height - y_high, :, :]
+        # 灏嗙┖鐧藉尯鍩熸坊鍔犲埌椤堕儴
+        png_img = np.concatenate((base, png_img), axis=0)
+    
     return png_img, y_high
 
 
diff --git a/hivision/creator/retinaface/inference.py b/hivision/creator/retinaface/inference.py
index 6f9d08c..40e47a9 100644
--- a/hivision/creator/retinaface/inference.py
+++ b/hivision/creator/retinaface/inference.py
@@ -46,17 +46,16 @@ keep_top_k = 750
 save_image = True
 vis_thres = 0.6
 
-ONNX_DEVICE = (
-    "CUDAExecutionProvider"
-    if onnxruntime.get_device() == "GPU"
-    else "CPUExecutionProvider"
+ONNX_DEVICE = onnxruntime.get_device()
+ONNX_PROVIDER = (
+    "CUDAExecutionProvider" if ONNX_DEVICE == "GPU" and "CUDAExecutionProvider" in onnxruntime.get_available_providers() else "CPUExecutionProvider"
 )
 
 
 def load_onnx_model(checkpoint_path, set_cpu=False):
     providers = (
         ["CUDAExecutionProvider", "CPUExecutionProvider"]
-        if ONNX_DEVICE == "CUDAExecutionProvider"
+        if ONNX_PROVIDER == "CUDAExecutionProvider"
         else ["CPUExecutionProvider"]
     )
 
@@ -67,14 +66,16 @@ def load_onnx_model(checkpoint_path, set_cpu=False):
     else:
         try:
             sess = onnxruntime.InferenceSession(checkpoint_path, providers=providers)
+            print(f"RetinaFace model loaded with providers: {sess.get_providers()}")
         except Exception as e:
-            if ONNX_DEVICE == "CUDAExecutionProvider":
-                print(f"Failed to load model with CUDAExecutionProvider: {e}")
+            if ONNX_PROVIDER == "CUDAExecutionProvider":
+                print(f"Failed to load RetinaFace model with CUDAExecutionProvider: {e}")
                 print("Falling back to CPUExecutionProvider")
                 # 灏濊瘯浣跨敤CPU鍔犺浇妯″瀷
                 sess = onnxruntime.InferenceSession(
                     checkpoint_path, providers=["CPUExecutionProvider"]
                 )
+                print(f"RetinaFace model loaded with providers: {sess.get_providers()}")
             else:
                 raise e  # 濡傛灉鏄疌PU鎵ц澶辫触锛岄噸鏂版姏鍑哄紓甯? 
diff --git a/hivision/plugin/beauty/handler.py b/hivision/plugin/beauty/handler.py
index dd165e6..2827c9a 100644
--- a/hivision/plugin/beauty/handler.py
+++ b/hivision/plugin/beauty/handler.py
@@ -1,4 +1,5 @@
 import cv2
+import numpy as np
 from hivision.creator.context import Context
 from hivision.plugin.beauty.whitening import make_whitening
 from hivision.plugin.beauty.base_adjust import (
@@ -42,7 +43,11 @@ def beauty_face(ctx: Context):
     if processed:
         # 鍒嗙涓棿鍥惧儚鐨凚GR閫氶亾
         b, g, r = cv2.split(middle_image)
-        # 浠庡師濮媘atting_image涓幏鍙朼lpha閫氶亾
-        _, _, _, alpha = cv2.split(ctx.matting_image)
-        # 鍚堝苟澶勭悊鍚庣殑BGR閫氶亾鍜屽師濮媋lpha閫氶亾
+        # 濡傛灉鏄烦杩囨姞鍥炬ā寮忥紝鍒涘缓涓€涓叏涓嶉€忔槑鐨刟lpha閫氶亾
+        if ctx.params.crop_only:
+            alpha = np.ones(b.shape, dtype=b.dtype) * 255
+        else:
+            # 浠庡師濮媘atting_image涓幏鍙朼lpha閫氶亾
+            _, _, _, alpha = cv2.split(ctx.matting_image)
+        # 鍚堝苟澶勭悊鍚庣殑BGR閫氶亾鍜宎lpha閫氶亾
         ctx.matting_image = cv2.merge((b, g, r, alpha))
